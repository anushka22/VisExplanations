\section{Method}
\label{sec:method}

In this section we propose a new method for automatically selecting good small multiple displays. Our approach takes three inputs from the analyst:
\begin{enumerate}
\item a visualization which the user wants to partition into a small multiple display,
\item a \textit{cognostic}~\cite{Tukey1982,Tukey1985} defined on this visualization type that measures the presence or absence of a visual pattern of interest to the user, and
\item a list of potential partitioning variables.
\end{enumerate}
The output is a scoring of the small multiple displays produced by each partitioning variable.

In the following section, we describe desirable properties for small multiple displays that we use to motivate our approach. The next section describes our method in the context of a running example. We then discuss some straightforward extensions to our basic approach.

\subsection{Goodness-of-Split Criteria}
To guide our approach, we formulated the following four goodness criteria. Partitioning variables should be chosen such that the resulting small multiple displays are:
\begin{itemize}
\item \emph{Visually rich}: We want small multiple displays that convey rich visual patterns, as captured by the cognostic provided by the analyst. In contrast to statistical methods, such as ANOVA, which are based on relatively simple summary metrics with closed-form distributions under some assumptions, most cognostics involve complicated processing and do not follow a known distribution.

\item \emph{Informative}: The purpose of a small multiple display is to help explain patterns in the input visualization. We want to prefer partitioning variables that add information to the display, supporting the user in their analysis. Partitions that randomly split the data are not useful since they don't contain any more information than the original plot.

\item \emph{Well-supported}: For some data sets, particularly those with outliers or with a small number of data points, strong visual patterns can occur by chance. These spurious patterns are misleading; they appear informative, but are not. We would like to detect and downweight such  patterns, guiding analysts to more robust results.

\item \emph{Parsimonious}: A small multiple display with many partitions can be very difficult to read and understand. All things being equal, we want to favor splitting into as few plots as possible, while still providing an informative display.
\end{itemize}

\subsection{Algorithm}

\begin{figure*}
 \centering 
    \begin{subfigure}[t]{1.35in}
        \includegraphics[width=1.35in]{images/AGE-MEDV.pdf}
        \caption{Input}
        \label{fig:method_original}
    \end{subfigure}
    \begin{subfigure}[t]{1.5in}
  	\includegraphics[width=1.5in]{images/RM.pdf}
	\caption{}
	 \label{fig:method_actual}
    \end{subfigure}
    \begin{subfigure}[t]{1.5in}
 	 \includegraphics[width=1.5in]{images/randCluster.pdf}
	 \label{fig:method_random}
 	\caption{}
    \end{subfigure}
     \begin{subfigure}[t]{1.9in}
 	\includegraphics[width=1.9in]{images/hist-RM.pdf}
	\caption{}
	 \label{fig:method_dist}
     \end{subfigure}
   \caption{Illustration of our method of evaluating small multiple displays. (a) Partitions determined by the levels of XX. (b)Randomly permuted partitions of data. (c) Distribution of X measures for randomly permuted partitions. The overlaid blue lines are the corresponding true scores of the partitions determined by XX in (a).}
\end{figure*}

Our approach is based on a simple intuition: effective small multiple displays are those whose component plots have cognostic scores that are very unlikely to have arisen due to chance. In this section, we describe a method for evaluating this likelihood using a \emph{randomized permutation test}, which is a non-parametric statistical significance test. In the following section, we demonstrate that this approach produces results that conform to our given goodness criteria.

Our algorithm works as follows: for a given partitioning variable, we evaluate the cognostic score on each component plot of the resulting small multiple, resulting in a vector of \emph{true} scores. We then repeatedly randomly permute the values of the partitioning variable, assigning each data point to a random partition, and reevaluate the cognostic score for each component plot. These randomized cognostic scores give us a vector of ``null distributions'', reflecting how likely different cognostic scores are to arise just by chance for each plot.

We then compare the true scores to the ``null distributions'' by evaluating a z-score. This gives us a normalized measure that indicates how far the true cognostic is from a random partition. Finally, to get a score for the whole small multiple display, we use the maximum absolute z-score across all the plots. 

To demonstrate this algorithm, consider the example in Figure~\ref{fig:method_original} showing the relationship between the median value of owner-occupied homes in $\$1000s$ and the proportion of owner-occupied units built prior to 1940. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Massachusetts~\cite{Harrison1978}. We see that as the age of houses increases, their median value decreases and there are more older houses in the Boston area

The analyst might wonder whether any of the other variables in the dataset could partition the view, isolating and explaining the skewness. So, to discover splits that are unusually skewed, the analyst selects the Skewed scagnostic~\cite{Wilkinson2005} as the non-parametric cognostic estimating the density of points in a scatterplot. 

Figure~\ref{fig:method_actual} shows the small multiple display resulting from partitioning on the binned measure of the property-tax rate per $\$10,000$. We compute it for each of the six component plots to form the vector of true scores. In the six partitions of equal count~\cite{Becker1996}, we see five that share the skewed pattern in the original view with a larger proportion of houses close to $100$ years old. However, for houses with property tax values in the range of $300-430$ we see a dispersed pattern with a lot more houses that are $50-60$ years old. Therefore, this variable reveals an informative partitioning of the original view as it diverges from the original.

We randomly permute the assignment of data points to the six partitions to produce a partitioning as in Figure~\ref{fig:method_random} where all the partitions look similar to the original bivariate relationship. We compute cognostic scores for each component plot. Then, we repeat this random permutated assignment and score computation to produce a distribution of cognostic scores for each component plot as seen in the Figure~\ref{fig:method_dist}. The true scores of the partitioning by the tax rate are overlaid as blue lines to establish a sense of the distance from random partitionings. We apply a non-parametric significance test to determine how significant the difference is. The z-score, for each component plot $p_i$, using Chebyshev's inequality is as follows:
$$z-score(p_i) = \frac{(x_i-\mu_i)^2}{\sigma_i^2}$$ (NOTE: not happy with these)
where $x_i$ is the true score of the $i$-th partition and $\mu_i$ and $\sigma_i$ are the mean and standard deviation of the cognostic measures over the $r$ random permutations for the $i$-th partition.

We then combine the z-scores from the $k$, here six, partitions to produce one value that serves as a ranking metric for the tax rate variable as follows: 
$$z-score = max_{i=1^k} (z-score(p_i))$$ 
We take the maximum of the z-scores from the partitions which ranks the tax rate variable at the top with a score of $4.144$. Combining scores with the sum would not penalize high-cardinality variables that result in many partitions. Similarly, combining scores with the mean would not help find outlier component plots. (NEED cleaning here)
(NEED: bootstrapping vs. permuting -- something about our sample = population)

\subsection{Extensions}
Handling Continuous Partitioners
Determining discrete splits for a categorical variable is trivial as the observations are naturally partitioned into subsets for each discrete choice the variable offers. For continuous variables, discrete partitions can be created through a binning technique. There are various binning techniques that split a continuous range into disjoint intervals~\cite{Freedman1981,Scott2009} employed in histograms. An alternative binning strategy is one with overlapping bins of roughly equal count called shingles~\cite{Becker1996}. The overlap of the intervals increases the resolution with which we can study conditional independence in the same way that moving averages increase the resolution of local behavior at each time point. 
%Figure~\ref{fig:shingles} shows such “moving snapshots” of the data across the range of the partitioning variable.

%NOT SURE IF THIS FITS HERE STILL OR SHOULD BE IN THE DISCUSSION
Small multiples or trellis plots facet a single view into multiple views combining partitioning variables using cross and nest operators~\cite{Wilkinson2005GG,Stolte2002}. Our algorithm could be used to progressively pick variables to partition on in the transition from a large single view~\cite{van2013} to informative small multiples.


