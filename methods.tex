\section{Method}
\label{sec:method}

We can handle the complexity of multi-dimensional data analysis by partitioning on a particular dimension to conditionally subset data based on its levels. Dividing data between views encodes the association between the subset of data observations in a partition using spatial proximity, which impacts the patterns that are visible. We call the resulting collection of partitions a Split. We start with a user-selected set of response dimensions often visualized in a single data view and seek a good Split determined by a conditioning dimension that influences the patterns seen in the original view. 

\subsection{Goodness-of-Split Criteria}
To compare Splits from different conditioning dimensions, we propose a set of goodness criteria to describe how well a Split explains the relationship between the selected response dimensions in the original view. The following criteria are generally applicable across view types:

\begin{itemize}
\item Visually Interesting Pattern: A visualization of a data relationship appears useful when it has a visually salient pattern that translates to some simple model of the relationship, for example, a linear trend. There are a number of measures to capture patterns of interest based the type of view being used and analytic task as mentioned in Section~\ref{sec:related}. A good Split slices data to reveal interesting visual patterns in the partitions.

\item Divergence: A good Split is one that reveals unexpected or different structure than that seen in the original view. Different visual patterns function as an indicator of the conditioning dimension's importance in explaining some of the hidden structure in the data relationship being studied and its independence relative to the response dimensions being studied.

\item Support: An indicator of the strength of the visible relationship is the proportion of data points that occur in the Split. It is easy to find patterns with small samples of random points, however, these are often not ``real" patterns in the context of the data population. This implies that we need to be more conservative at interpreting visual patterns in splits consisting of small sets of points or low support. A good Split has high support for the visual patterns it describes.

\item Degrees of Freedom: The number of partitions in a Split is the number of components that fully determine the conditioning dimension's effect on the data being modeled. As the number of partitions increase, the user has more visual comparisons to make and the overall model describing the high-dimensional data structure has more potential interaction levels. Also, the degrees of freedom inversely affects the support criterion as the number of points per partition would decrease as the number of partitions increase given the constant number of data observations. A good Split has fewer degrees of freedom allowing for simpler models with potentially more support.
\end{itemize}

\subsection{Algorithm}
Given a user-selected set of response dimensions, we want to automatically find a good Split respecting the criteria outlined above. This process of selecting a good Split, adding a conditioning dimension to explain the visual structure in a set of data observations when we have many potential such dimensions, is akin to the model selection process. We evaluate the ``model" a Split determines based not only on the quality of the interesting patterns in partitions, but also, on the simplicity or size of the collection of partitions. Model selection is commonly employed in statistics and data mining to pick a statistical model that fits a sample of data, such as curve fitting or clustering. However, applying such a methodology with cognostics or visual pattern measures studied in the information visualization community has not been explored.

We describe an algorithm to apply cognostics to do model selection where a model is determined as the addition of an explanatory dimension to a selected set of response dimensions. We employ the idea of permutation tests which is a non-parametric approach to establish the null distribution of a test statistic. The approach is to compute the test statistic on the original observations, rearrange them, then compute the test statistic on the rearranged data and compare the two computed test statistics to check if they are significantly different. Here, we find the statistical significance of a particular Split using a cognostic as the test statistic. This allows us to evaluate conditioning dimensions and automatically find a good Split that reveals patterns that may not be due to chance.

We describe our algorithm, as it relates to the goodness-of-split criteria, with a running example examining data about American universities. The Integrated Postsecondary Education Data System (IPEDS) is the primary source for data on colleges, universities, and technical and vocational postsecondary institutions in the United States via the National Center for Education Statistics.\footnote{\url{http://public.tableau.com/s/resources?qt-overview_resources=1}} For simplicity, we describe a scenario where the user selects a bivariate relationship of interest - that between the percent of admitted students and the graduation rate of students with Bachelors degrees within $6$ years as seen in Figure~\ref{fig:teaserAll}. The proposed algorithm could be applied to other view types and associated quality metrics. We walk through evaluating the goodness of the Split resulting from using the partitioning variable

\subsubsection{Visually Interesting Pattern}
Central to our approach is the use of a cognostic that tracks the visually interesting pattern we seek to distinguish in a Split. The cognostic is an independent, substitutable piece of the approach and its choice is based on the user's task and interest.

For ANOVA-type analytic tasks we could pick a cognostic that considers the mean and variance so that a good Split would have partitions that were relatively homogenous and with distinct means. Correlation or statistics from a linear regression fit of linear could be used to find a Split with strong trends. Entropy-based measures on image space find visually salient patterns but are unable to distinguish a sine wave patterns from a Gaussian pattern if the points fall in the same proportion of bins. To differentiate visual distinct patterns, we could employ scagnostics to non-parametrically characterize the distributional shape of visual patterns in a scatterplot. For our example, we use the Monotonic scagnostic as our cognostic.

\subsubsection{Divergence}
We capture divergence by comparing the cognostic measure from a selected Split to the same measure computed on random similar Splits. Let the partitioning dimension $d_p$ create $k$ partitions with sizes $\left\{ {s_1, s_2,...,s_k}\right\}$. We compute the cognostic on each of these $k$ partitions.
Then, we generate $r$ random Splits or sets of $k$ partitions sampled from the original data and of the same sizes $\left\{ {s_1, s_2,...,s_k}\right\}$. We use permutation sampling as we assume our dataset is representative of the population and do not need to compensate, via bootstrapping, for the assumption that it represents a sample from a larger, unexamined population. <NEED MORE HERE> We compute the scagnostic on each of these $k$ random partitions and then build up a distribution of cognostics from the $r$ random samples. We then compare the cognostic measures from each of the $k$ partitions created by $d_p$ to the cognostic distribution corresponding to the randomly permuted sample partition of the same size.

\subsubsection{Support}
The support criterion is triggered in how we determine if the difference in cognostic measures between the Partitioner splits and random splits is meaningful. The generated measure distributions function as reference "null distributions" and we apply Chebyshev's inequality to determine how significant the difference of the Partitioner's splits are from random. To account for the observation that the distributions for various cognostics were not Gaussian, we use a non-parameasure significance test. Now, for splits of small size $s_i$, there will be a wide spread of values for most cognostics as it is easy to find a pattern with a few points. However, these patterns are not ``real" or significant patterns because of low support and we expect the cognostics for the splits from Partitioner dimension to fall within the reference distribution from random splits. This implies that we will be conservative in accepting visual patterns from splits with low support as truly revealing interesting structure. Conversely, cognostics with high support are likely to be meaningful and a significant difference from the reference distribution would mark a split with interesting visual patterns.

\subsubsection{Degrees of Freedom}
We want to penalize a large number of splits as this negatively affects the support and it provides a heavier investment from the user in terms of visual comparisons. The z-score from applying Chebyshev's inequality  penalizes the increase in number of splits $k$ (as $k$ goes to $\infty$, the z-score goes to $0$)
$$\sum_{i=1}^n \frac{(X_i-\mu_i)^2}{\sigma_i^2}$$

\subsection{Handling Continuous Partitioners}
Determining discrete splits for a categorical dimension is trivial as the observations are naturally partitioned into subsets for each discrete choice the dimension offers. For continuous dimensions, discrete partitions can be created through a binning technique. There are various binning techniques that split a continuous range into disjoint intervals~\cite{Freedman1981,Scott2009} employed in histograms. An alternative binning strategy is one with overlapping bins of roughly equal count called shingles~\cite{Becker1996 ?Cleveland book first??}. The overlap of the intervals increases the resolution with which we can study conditional independence in the same way that moving averages increase the resolution of local behavior at each time point. e “moving snapshots” of the data across the range of the conditioning dimension

\subsection{Multiple Partitioners}
Small multiples or trellis plots facet a single view into multiple views, each displaying subsets of data conditioned on the dimensions defining the small multiple. These conditioning or Partitioner dimensions are combined using cross and nest operators~\cite{Wilkinson2005GG,Stolte2002} to define the layout of the small multiple. Our algorithm could be used to pick Partitioner dimensions in the transition from a large single view~\cite{van2013} to informative small multiples.


