\section{Method}
Statistical methods like ridge and lasso regression help automatically select subsets of variables that produce good explanatory models of a set of multivariate observations. However, these methods make a number of assumptions about the errors in the model given the sample and about the independence of predictor variables. Using lasso regression to progressively add explanatory variables assumes interest in a linear model and may not translate to different, visually interesting  patterns in consecutive steps. Also, these regression methods offer globally optimal solutions.

Given a user-selected data relationship that captures the set of dependent variables of interest, we seek to add explanatory variables that help explain the patterns seen in the visual representations. For simplicity, we describe a scenario where the user selects a bivariate relationship of interest and visualizes it as a scatterplot. The proposed method could be applied to other view types and associate quality metrics. Adding explanatory variables partitions the scatterplot into multiple plots (small multiples) such that there as many plots as there are discrete categories of a categorical Partitioner or discrete bins of a quantitative Partitioner variable. Therefore, a Partitioner splits or facets a particular visual representation into similar views of subsets of the data. We propose a number of criteria to evaluate the goodness of the splits resulting from adding a Partitioner variable to explain the structure seen in the original visual representation.

\subsection{Split Criteria}
We set up the following goodness-of-split criteria to guide our work:
\begin{itemize}
\item Visually Interesting Pattern: A plot of a data relationship appears useful when it has a visually salient pattern that translates to some simple model of the relationship, for example, a linear trend. There are number of measures to capture patterns of interest based the type of plot being used for visual analysis. A Partitioner variable that produces splits with visually salient patterns, which are potentially useful, showcases a way of slicing the data relationship to reveal interesting structure.
\item Divergence: The Partitioner variable of particular interest is one that reveals unexpected or different structure than that seen in the large single view. Different visual patterns function as an indicator of the Partitioner's importance in explaining some of the hidden structure in the data relationship being studied, therefore, making the result a good split. Similarly and consequently the visual patterns across plots in the resulting split would be diverse.
\item Support: An indicator of the strength of the relationship or visual pattern is the proportion of data points that occur in the split and contribute to the pattern. It is easy to find patterns with small samples of random points, however, these are often not ``real" patterns in the context of the data population. This implies that we need to be more conservative at interpreting visual patterns in splits constituting of small samples of points.
\item Degrees of Freedom: The number of splits captures the dimension of the domain as it is the number of components that fully determine the Partitioner's effect on the data being modeled. As the number of splits increase, the user has more visual comparisons to make and the overall model describing the high-dimensional data structure has more potential interaction levels. Also, the degrees of freedom affects the support criterion as the number of points per split would decrease as the number of splits increase given a constant number of observations to start with. Lower degrees of freedom allow for simpler models with potentially more support.
\end{itemize}

\subsection{Algorithm}
We describe the design of an algorithm that constructs good splits given the criteria outlined above. We assume we start with the user specified visual representation of a set of data and seek to facet the display into splits that reveal useful visual structure.

\subsubsection{Visual Patterns}
Filtering the large number of views of a high-dimensional dataset motivated Tukey's proposal of \textit{cognostics}~\cite{Tukey1982,Tukey1985} - diagnostic measures to evaluate the usefulness of views - so users would only manually investigate a small set of high-ranked, potentially useful views. As described in Section~\ref{sec:related}, there are numerous measures to evaluate various view types (from scatterplots to radial views) based on various tasks (from finding outliers to separating classes or groups).

We can explain the distribution of observations by splitting the dataset into groups based on the Partitioner such that each group is relatively homogenous (has low variance) and the mean of each group is distinct. Then we could apply ANOVA to we compare the means of these groups and look for significant differences as indicators of interesting splits. This type of analysis assumes that the groups being compared are statistically independent and are balanced in size which will not be the case for arbitrary categorical fields in datasets. 

Another visual pattern measure could be to use the correlation or slopes from linear regression fits to help distinguish splits where subsets of the data determined by the Partitioner variable have particular linear relationships. This could help in the discovery of confounding covariates, the unexamined fields that have an effect on the data pattern. Simpson's paradox is a classic example of such mix-effects when aggregate numbers are affected by changes in the relative size and value of the subpopulations. 

Going towards non-parametric measures, visual pattern salience is often captured by entropy on the binned visual representation. However, entropy does not consider the adjacency pattern in the grid of points so a sine wave pattern might be just as interesting a small Gaussian pattern if they sit in the same number of bins. Here, we would like to be able to differentiate visual distinct patterns.

The distributional shape of visual patterns in a scatterplot are quantitatively captured through graph-theoretic scagnostics~\cite{Wilkinson2005}. These measures have the benefit of being non-parametric and robust to the number of points as they first bin the data. However, when considering robustness, these measures are not altogether scale-invariant nor do they capture location specifics if we are interested in particular positional patterns. Selecting non-metric measures for the visual pattern would allow for a more generally applicable algorithm. 

\subsubsection{Divergence}
After selecting a cognostic measure that quantitatively captures the visual pattern of a set of data observations, we use the measure to determine a set of scores for the set of splits that result from applying the facet operator with a particular Partitioner variable. The divergence requirement is captured by comparing the set of scores to a similar set of scores for random splits. Let the Partitioner variable $d_p$ create $k$ splits with sizes $\left\{ {s_1, s_2,...,s_k}\right\}$. We generate $r$ random sets of $k$ splits sampled from the original data and of the same sizes $\left\{ {s_1, s_2,...,s_k}\right\}$. We use permutation sampling as we consider the dataset representative of the population and do not need to compensate, via bootstrapping, for the assumption that it represents a sample from a larger, unexamined population. From these sampled random splits we compute the cognostic on each split and get $k$ distributions of cognostics. We then compare the cognostic measures from the Partitioner split to these random splits.

\subsubsection{Support}
The support criterion is triggered in how we determine if the difference in cognostic measures between the Partitioner splits and random splits is meaningful. The generated measure distributions function as reference "null distributions" and we apply Chebyshev's inequality to determine how significant the difference of the Partitioner's splits are from random. To account for the observation that the distributions for various cognostics were not Gaussian, we use a non-parameasure significance test. Now, for splits of small size $s_i$, there will be a wide spread of values for most cognostics as it is easy to find a pattern with a few points. However, these patterns are not ``real" or significant patterns because of low support and we expect the cognostics for the splits from Partitioner variable to fall within the reference distribution from random splits. This implies that we will be conservative in accepting visual patterns from splits with low support as truly revealing interesting structure. Conversely, cognostics with high support are likely to be meaningful and a significant difference from the reference distribution would mark a split with interesting visual patterns.

\subsubsection{Degrees of Freedom}
We want to penalize a large number of splits as this negatively affects the support and it provides a heavier investment from the user in terms of visual comparisons. The z-score from applying Chebyshev's inequality  penalizes the increase in number of splits $k$ (as $k$ goes to $\infty$, the z-score goes to $0$)
$$\sum_{i=1}^n \frac{(X_i-\mu_i)^2}{\sigma_i^2}$$

\subsection{Handling Continuous Partitioners}
Determining discrete splits for a categorical variable is trivial as the observations are naturally partitioned into subsets for each discrete choice the variable offers. For continuous variables, discrete partitions can be created through a binning technique. There are various binning techniques that split a continuous range into disjoint intervals~\cite{Freedman1981,Scott2009} employed in histograms. An alternative binning strategy is one with overlapping bins of roughly equal count called shingles~\cite{Becker1996 ?Cleveland book first??}. The overlap of the intervals increases the resolution with which we can study conditional independence in the same way that moving averages increase the resolution of local behavior at each time point.

\subsection{Multiple Partitioners}
Small multiples or trellis plots facet a single view into multiple views, each displaying subsets of data conditioned on the variables defining the small multiple. These conditioning or Partitioner variables are combined using cross and nest operators~\cite{Wilkinson2005GG,Stolte2002} to define the layout of the small multiple. Our algorithm could be used to pick Partitioner variables in the transition from a large single view~\cite{van2013} to informative small multiples.


