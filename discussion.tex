\section{Discussion and Future Work}
\label{sec:discussion}
(Answer potential reviewer's question about how this relates to stepwise regression type methods + feature selection in other methods)
Statistical methods like ridge and lasso regression help automatically select subsets of variables that produce good explanatory models of a set of multivariate observations. Using lasso regression to progressively add explanatory variables assumes interest in a linear model and may not translate to different, visually interesting  patterns in consecutive steps. These methods build up joint models while we are look at pairwise models considering adding one explanatory variable independent of the rest. 

One weakness with our approach is that we do not correct for possible correlation between the patterns in the input visualization and partitioning variables. As a result, we may redundantly choose a small multiple display that shows a pattern that was already clearly visible in the original plot. While exposing highly correlated variables can be useful, it is likely not what the user wants in an effective small multiple display. Statistical methods for variable selection, such as ridge or lasso regression, can downweight highly correlated variables. Our approach would be improved by incorporating similar behavior. 

There is more work to be done in this area. Non-parametric approaches allow us to use quality metrics without a closed form distribution, which seems essential in evaluating visual patterns. However, these approaches are computational demanding due to the need to recompute the metric on a large number of samples. More work on computationally efficient visual metrics is needed. Also, in our work with Scagnostics, we have found that they sometimes miss very obvious visual patterns. More work is needed to develop cognostics that are robust to properties such as sample size, the amount of noise in the data set, the location or scale of the axes, etc. 

Our method currently only considers a single partitioning (by one or more variables). We could extend our approach to consider sequences of partitions. This could be used to develop a decision tree based exploratory data analysis interaction mechanism guided by our algorithm. At each decision level, we could apply our algorithm to select a partitioning variable given a single view of the data at that level. This would produce a small multiple display where each component plot could be further partitioned to reveal interesting structure. Considering the tree structure, each choice of a partitioning variable would be conditional on the other previously used variables, as in model selection methods. 

Finally, while we considered creating small multiple displays by partitioning data, another common use case is creating small multiples by drilling down into aggregated data. A variation of our approach could be used to detect if potentially interesting visual information would be revealed by a change in level of detail. Visualization tools could use this to recommend a drill down or roll up dimension.

